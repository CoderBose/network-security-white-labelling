{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "400721d4-c665-4547-8f59-1636ebc1476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import urllib\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "import pandas as pd\n",
    "import http\n",
    "\n",
    "#Lib require for google drive\n",
    "from io import BytesIO\n",
    "from googleapiclient.http import MediaFileUpload, MediaIoBaseUpload\n",
    "from pydrive2.auth import GoogleAuth\n",
    "from pydrive2.drive import GoogleDrive\n",
    "from pydrive2.files import GoogleDriveFile\n",
    "import mimetypes\n",
    "import magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "fa751ed9-806f-435a-9104-4116120b798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agents = ['Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/37.0.2062.94 Chrome/37.0.2062.94 Safari/537.36',\n",
    " 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.85 Safari/537.36',\n",
    " 'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    " 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:40.0) Gecko/20100101 Firefox/40.0',\n",
    " 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/600.8.9 (KHTML, like Gecko) Version/8.0.8 Safari/600.8.9',\n",
    " 'Mozilla/5.0 (iPad; CPU OS 8_4_1 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) Version/8.0 Mobile/12H321 Safari/600.1.4',\n",
    " 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.85 Safari/537.36',\n",
    " 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.85 Safari/537.36',\n",
    " 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.10240',\n",
    " 'Mozilla/5.0 (Windows NT 6.3; WOW64; rv:40.0) Gecko/20100101 Firefox/40.0',\n",
    " 'Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    " 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.85 Safari/537.36',\n",
    " 'Mozilla/5.0 (Windows NT 6.1; Trident/7.0; rv:11.0) like Gecko',\n",
    " 'Mozilla/5.0 (Windows NT 10.0; WOW64; rv:40.0) Gecko/20100101 Firefox/40.0',\n",
    " 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_4) AppleWebKit/600.7.12 (KHTML, like Gecko) Version/8.0.7 Safari/600.7.12',\n",
    " 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.85 Safari/537.36',\n",
    " 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.10; rv:40.0) Gecko/20100101 Firefox/40.0',\n",
    " 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/600.8.9 (KHTML, like Gecko) Version/7.1.8 Safari/537.85.17',\n",
    " 'Mozilla/5.0 (iPad; CPU OS 8_4 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) Version/8.0 Mobile/12H143 Safari/600.1.4',\n",
    " 'Mozilla/5.0 (iPad; CPU OS 8_3 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) Version/8.0 Mobile/12F69 Safari/600.1.4',\n",
    " 'Mozilla/5.0 (Windows NT 6.1; rv:40.0) Gecko/20100101 Firefox/40.0',\n",
    " 'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)',\n",
    " 'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)',\n",
    " 'Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; Touch; rv:11.0) like Gecko',\n",
    " 'Mozilla/5.0 (Windows NT 5.1; rv:40.0) Gecko/20100101 Firefox/40.0',\n",
    " 'Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.85 Safari/537.36',\n",
    " 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_3) AppleWebKit/600.6.3 (KHTML, like Gecko) Version/8.0.6 Safari/600.6.3',\n",
    " 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_3) AppleWebKit/600.5.17 (KHTML, like Gecko) Version/8.0.5 Safari/600.5.17',\n",
    " 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:38.0) Gecko/20100101 Firefox/38.0',\n",
    " 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\n",
    " 'Mozilla/5.0 (iPhone; CPU iPhone OS 8_4_1 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) Version/8.0 Mobile/12H321 Safari/600.1.4',\n",
    " 'Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    " 'Mozilla/5.0 (iPad; CPU OS 7_1_2 like Mac OS X) AppleWebKit/537.51.2 (KHTML, like Gecko) Version/7.0 Mobile/11D257 Safari/9537.53',\n",
    " 'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)',\n",
    " 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.85 Safari/537.36',\n",
    " 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.85 Safari/537.36',\n",
    " 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:40.0) Gecko/20100101 Firefox/40.0',\n",
    " 'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/6.0)',\n",
    " 'Mozilla/5.0 (Windows NT 6.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.85 Safari/537.36',\n",
    " 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\n",
    " 'Mozilla/5.0 (X11; CrOS x86_64 7077.134.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.156 Safari/537.36',\n",
    " 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/600.7.12 (KHTML, like Gecko) Version/7.1.7 Safari/537.85.16',\n",
    " 'Mozilla/5.0 (Windows NT 6.0; rv:40.0) Gecko/20100101 Firefox/40.0',\n",
    " 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:40.0) Gecko/20100101 Firefox/40.0',\n",
    " 'Mozilla/5.0 (iPad; CPU OS 8_1_3 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) Version/8.0 Mobile/12B466 Safari/600.1.4',\n",
    " 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_2) AppleWebKit/600.3.18 (KHTML, like Gecko) Version/8.0.3 Safari/600.3.18',\n",
    " 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.85 Safari/537.36',\n",
    " 'Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.85 Safari/537.36',\n",
    " 'Mozilla/5.0 (Windows NT 6.1; Win64; x64; Trident/7.0; rv:11.0) like Gecko',\n",
    " 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbf9643f-b6b6-4e45-85dc-7471702cae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_url_params(url, kwargs):\n",
    "    \"\"\"\n",
    "        Construct new url by adding query params to the url.\n",
    "\n",
    "        Returns: new url\n",
    "    \"\"\"\n",
    "\n",
    "    #if no query params add ? else add & at the end of url\n",
    "    url+='?' if not '?' in url else '&'\n",
    "    for i,j in kwargs.items():\n",
    "        url+=f'{i}={j}&'\n",
    "\n",
    "    #strip extra & at the end\n",
    "    return url.rstrip('&')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fe8aa3b1-df87-40fb-8995-1e31989d5c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleDriveWrapper:\n",
    "    def __init__(self):\n",
    "        auth = GoogleAuth()\n",
    "\n",
    "        # Try to load saved client credentials\n",
    "        auth.LoadCredentialsFile(\"creds.txt\")\n",
    "        if auth.credentials is None:\n",
    "            # Authenticate if they're not there\n",
    "            auth.GetFlow()\n",
    "            auth.flow.params.update({'access_type': 'offline'})\n",
    "            auth.flow.params.update({'approval_prompt': 'force'})\n",
    "            auth_url = auth.GetAuthUrl()\n",
    "\n",
    "            auth.LocalWebserverAuth()\n",
    "        elif auth.access_token_expired:\n",
    "            # Refresh them if expired\n",
    "            auth.Refresh()\n",
    "        else:\n",
    "            # Initialize the saved creds\n",
    "            auth.Authorize()\n",
    "        # Save the current credentials to a file\n",
    "        auth.SaveCredentialsFile(\"creds.txt\")\n",
    "\n",
    "        self.drive = GoogleDrive(auth)\n",
    "        \n",
    "    def getFileID(self, file_name, parent_id=None, url=False):\n",
    "        \"\"\"\n",
    "            Function to get file id by name, if file is not present, return None\n",
    "        \"\"\"\n",
    "        query = f\"mimeType != 'application/vnd.google-apps.folder' and title='{file_name}'\"\n",
    "        if parent_id:\n",
    "            query+= f\" and '{parent_id}' in parents\"\n",
    "\n",
    "        file_list = self.drive.ListFile({'q':query}).GetList()\n",
    "        for file in file_list:\n",
    "            if url:\n",
    "                return file['alternateLink']\n",
    "            return file['id']\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def getFolderID(self, folder_name, parent_id=None, url=False):\n",
    "        \"\"\"\n",
    "            Function to get folder id by name, if folder is not present, return None\n",
    "        \"\"\"\n",
    "        \n",
    "        #create the query to list the folder\n",
    "        query = f\"mimeType = 'application/vnd.google-apps.folder' and title='{folder_name}'\"\n",
    "        \n",
    "        if parent_id:\n",
    "            query+= f\" and '{parent_id}' in parents\"\n",
    "\n",
    "        file_list = self.drive.ListFile({'q':query}).GetList()\n",
    "        for file in file_list:\n",
    "            if url:\n",
    "                return file['alternateLink']\n",
    "            return file['id']\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def createFolder(self, folder_name, parent_id=None):\n",
    "        \"\"\"\n",
    "            Function to create a folder under a folder (if parent_id is present), return folder_id\n",
    "        \"\"\"\n",
    "        #check if folder exists\n",
    "        folder_id = self.getFolderID(folder_name, parent_id)\n",
    "        \n",
    "        if folder_id:\n",
    "            return folder_id\n",
    "        \n",
    "        # folder not present, create the folder\n",
    "        folder_metadata = {\n",
    "            'title': folder_name,\n",
    "            'mimeType': 'application/vnd.google-apps.folder',\n",
    "        }\n",
    "        \n",
    "        if parent_id:\n",
    "            folder_metadata['parents'] = [{'id': parent_id}]\n",
    "        \n",
    "        folder = self.drive.CreateFile(folder_metadata)\n",
    "        folder.Upload()     \n",
    "        \n",
    "        return folder['id']\n",
    "    \n",
    "    \n",
    "    \n",
    "    def uploadFile(self, file_name, file_path=None, file_url=None, parent_id=None):\n",
    "        \"\"\"\n",
    "            Function to upload File, \n",
    "            \n",
    "            file_name: Name of the file to upload\n",
    "            file_path: Upload the file on file path\n",
    "            file_url: Upload a file from a url\n",
    "            parent_id: Parent Folder Id\n",
    "            \n",
    "            Either Filepath or File_url is required to upload, if both present, picks file_path over file_url\n",
    "        \"\"\"\n",
    "    \n",
    "        # file metadata to upload\n",
    "        file_metadata = {\n",
    "            'title': file_name,\n",
    "            'parents': [{'id': parent_id}] if parent_id else []\n",
    "        }\n",
    "\n",
    "        #upload a file from file_path\n",
    "        if file_path:\n",
    "            #get the mime_type of file\n",
    "            file_metadata['mime_type'] = mimetypes.guess_type(file_path)[0]\n",
    "            \n",
    "            #create the file and set the content of file\n",
    "            media = self.drive.CreateFile(metadata=file_metadata)\n",
    "            media.SetContentFile(filename=file_path)\n",
    "\n",
    "        elif file_url:\n",
    "            \n",
    "            # Make request to load the file from url\n",
    "            headers = {'User-Agent': random.choice(user_agents)}\n",
    "            req = urllib.request.Request(file_url, headers=headers)\n",
    "\n",
    "            #if file size is greater, file loads partially\n",
    "            max_retries = 100\n",
    "            retry_count = 0\n",
    "            remote_file = b\"\"\n",
    "\n",
    "            while retry_count <= max_retries:\n",
    "                try:\n",
    "                    #request to load the file\n",
    "                    response = urllib.request.urlopen(req)\n",
    "                    \n",
    "                    #if request is succefull, read the file response and break the loop\n",
    "                    remote_file = response.read()\n",
    "                    break\n",
    "                except http.client.IncompleteRead as e:\n",
    "                    #if incompleteread exception, load the file partially \n",
    "                    remote_file += e.partial\n",
    "                    retry_count += 1\n",
    "                    print(f\"IncompleteRead error. Retrying... (attempt {retry_count}/{max_retries})\")\n",
    "                    \n",
    "                except urllib.error.HTTPError as err:\n",
    "                    if err.status == 429:\n",
    "                        #exponential backoff time\n",
    "                        delay = min(5*2**retry_count, 40)\n",
    "                        if delay > 40:\n",
    "                            break\n",
    "                            \n",
    "                        \n",
    "                        print(f\"HTTP Error 429: sleeping for {delay} seconds!!\")\n",
    "                        \n",
    "                        time.sleep(delay)\n",
    "                        retry_count += 1\n",
    "                    else:\n",
    "                        print(err)\n",
    "                        break\n",
    "                except Exception as err:\n",
    "                    print(err)\n",
    "                    break\n",
    "\n",
    "            #if no content, return None\n",
    "            if not remote_file:\n",
    "                return None\n",
    "\n",
    "\n",
    "            #get the mime type of a file\n",
    "            file_metadata['mime_type'] = mimetypes.guess_type(file_name)[0]\n",
    "            \n",
    "            #create the file and convet the file to BytesIO since drive requires to read file in string where it encodes the file\n",
    "            media = self.drive.CreateFile(metadata=file_metadata)\n",
    "            media.content = BytesIO(remote_file)\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Required either file_path or file_url\")\n",
    "\n",
    "        #uplaod the file\n",
    "        media.Upload()\n",
    "\n",
    "        print(f\"File '{file_name}' has been uploaded to Google Drive.\")\n",
    "\n",
    "        #return the web link\n",
    "        return media['alternateLink']\n",
    "\n",
    "def guessMIMEType(file_path = None, memory_file=None):\n",
    "    try:\n",
    "        #return the mimetype for filepath\n",
    "        if file_path:\n",
    "            return mimetypes.guess_type(file_path)[0]\n",
    "        \n",
    "        #find the mimetype for file stored in memory\n",
    "        elif memory_file:\n",
    "            mime = magic.Magic(mime=True)\n",
    "            mime_type = mime.from_buffer(memory_file.read())\n",
    "\n",
    "            memory_file.seek(0)  # Reset the file pointer after reading the content\n",
    "\n",
    "            return mime_type\n",
    "        else:\n",
    "            raise Exception(\"Required either file_path or memory_file\")\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        return 'application/octet-stream'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de5be46d-22d5-40f1-9447-dadaf8d4326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WalmartScraper:\n",
    "    base_url = \"https://www.walmart.com/search?q=\"\n",
    "\n",
    "    def __init__(self, product_name, chrome_path=r\"C:\\Users\\Priyanshi Singh\\Downloads\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe\"):\n",
    "\n",
    "        service = webdriver.chrome.service.Service()#executable_path=chrome_path)\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"--start-maximized\")\n",
    "        options.add_argument(f'--user-agent={random.choice(user_agents)}')\n",
    "        options.add_argument(\"--disable-blink-features=AutomationControlled\") \n",
    "        # Exclude the collection of enable-automation switches \n",
    "        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"]) \n",
    "        options.add_experimental_option(\"detach\", False) \n",
    "        \n",
    "        # Turn-off userAutomationExtension \n",
    "        options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "        self.driver = webdriver.Chrome(service=service, options=options)\n",
    "        self.search_url = self.base_url+ re.sub(\"\\s+\", \"+\", product_name)\n",
    "        self.driver.get(self.search_url)\n",
    "\n",
    "\n",
    "    def getProductURLList(self):\n",
    "        # Wait for the pagination element to be present\n",
    "        WebDriverWait(self.driver, 20).until(EC.presence_of_element_located((By.XPATH, \"//div[@id='results-container']\")))\n",
    "\n",
    "        total_pages = int(self.driver.find_elements(By.XPATH, \"//nav[@aria-label='pagination']//li[not(@area-hidden) and div]\")[-1].text)\n",
    "        product_urls = []\n",
    "\n",
    "        # Loop through each page\n",
    "        for curr_page in range(1, total_pages + 1):\n",
    "            # Get the product URLs on the current page\n",
    "            products = self.driver.find_elements(By.XPATH, \"//div[@data-item-id]//a\")\n",
    "\n",
    "            for product in products:\n",
    "                product_urls.append(product.get_attribute(\"href\"))\n",
    "\n",
    "            # Go to the next page if not on the last page\n",
    "            if curr_page < total_pages:\n",
    "                next_page_url = construct_url_params(self.search_url, {'page': curr_page + 1})\n",
    "                self.driver.get(next_page_url)\n",
    "        return product_urls\n",
    "\n",
    "    def getProductNameAndIDFromURL(self):\n",
    "        product_path = urlparse(self.driver.current_url).path.strip('/').split('/')\n",
    "        product_name = product_path[-2].replace(\"-\", \" \")\n",
    "        product_id = product_path[-1]\n",
    "        return product_name, product_id\n",
    "   \n",
    "    def getLeftImageUrls(self):\n",
    "        # Find all image elements within the left panel\n",
    "        thumbnails = self.driver.find_elements(By.XPATH, \"//div[@data-testid='vertical-carousel-container']//img\")\n",
    "        # Extract and return the 'src' attribute from each image\n",
    "        image_urls = [thumbnail.get_attribute('src') for thumbnail in thumbnails]\n",
    "        # Parse each image URL using urlparse and create a new list of formatted URLs\n",
    "        formatted_urls = [f\"{url_res.scheme}://{url_res.netloc}/{url_res.path}\" for image_url in image_urls for url_res in map(urlparse, [image_url])]\n",
    "        return formatted_urls\n",
    "\n",
    "    def parseWalmartProduct(self):\n",
    "        product_detail = {}\n",
    "    \n",
    "        # Brand Name\n",
    "        brand_element = self.driver.find_element(By.XPATH, \"//section[@data-pcss-show='true']//a\")\n",
    "        product_detail['product_brand'] = brand_element.text\n",
    "    \n",
    "        # Product Name\n",
    "        product_name_element = self.driver.find_element(By.XPATH, \"//h1[@id = 'main-title']\")\n",
    "        product_detail['product_title'] = product_name_element.text\n",
    "    \n",
    "        # Rating Number\n",
    "        rating_element = self.driver.find_elements(By.XPATH, \"//div[@data-testid='reviews-and-ratings']//span[contains(@class, 'rating-number')]\")\n",
    "        \n",
    "        product_detail['rating_number'] = rating_element[0].text.replace('(','').replace(')', '') if rating_element else ''\n",
    "\n",
    "        price_element = self.driver.find_elements(By.XPATH, \"//span[@itemprop='price']\")\n",
    "        product_detail['price'] = price_element[0].text if price_element else ''\n",
    "\n",
    "        return product_detail\n",
    "\n",
    "    def parseWalmartProductDetails(self):\n",
    "        walmart_product_detail = {}          \n",
    "        # Parse product details\n",
    "        product_details_section = self.driver.find_elements(By.XPATH, \"//section[@aria-describedby]\") #//div[@data-testid='product-description-content']\")\n",
    "        \n",
    "        for i, section in enumerate(product_details_section):\n",
    "            headers = section.find_elements(By.XPATH, \".//div[contains(@class, 'expand-collapse-header')]\")\n",
    "            header =  headers[0].text.strip() if headers else None\n",
    "            if section.find_elements(By.XPATH, \".//div[@data-testid='product-description-content']\"):\n",
    "                #product description div\n",
    "                divs = section.find_elements(By.XPATH, \".//div[@class='mb3']\")\n",
    "        \n",
    "                walmart_product_detail['short_description'] = divs[0].text.strip()\n",
    "        \n",
    "                long_description = ''\n",
    "                for div in divs[1:]:\n",
    "                    inner_div  = div.find_elements(By.XPATH, \".//div[@class='dangerous-html mb3']\")\n",
    "        \n",
    "                    inner_div_header = None\n",
    "                    if inner_div:\n",
    "                        #next tag is bold\n",
    "                        inner_div_header = inner_div[0].find_elements(By.XPATH, \"./b\")\n",
    "        \n",
    "                \n",
    "                    inner_div_header = inner_div_header[0].text.rstrip(':') if inner_div_header else None\n",
    "                    if inner_div_header == 'Key Features':\n",
    "                        #get all divs with class dangerous-html mb3, get immediate next div, get all divs under it which has some inner TAG\n",
    "                        feature_divs = div.find_elements(By.XPATH, \".//div[@class='dangerous-html mb3']/div//div[./*]\")\n",
    "        \n",
    "                        for feature_div in feature_divs:\n",
    "                            if feature_div.text.strip(':')=='Specifications':\n",
    "                                #now get all the next div tags till the div tag does not have any innner tag\n",
    "                                for spec in feature_div.find_elements(By.XPATH, \"following-sibling::div[not(div)]\"):\n",
    "                                    features =  spec.text.strip().split(':')\n",
    "                                    if features:\n",
    "                                        walmart_product_detail[re.sub('^\\d+\\.', '', features[0])] = features[1]\n",
    "        \n",
    "                                    else:\n",
    "                                        #add to long description\n",
    "                                        long_description += '\\n' + spec.text.strip()\n",
    "        \n",
    "                            else:\n",
    "                                long_description += '\\n' + feature_div.text.strip()\n",
    "                    else:\n",
    "                        \n",
    "                        long_description += \"\\n\" + div.text.strip()\n",
    "        \n",
    "                walmart_product_detail['long_description'] = long_description\n",
    "            elif header == \"Specifications\":\n",
    "                div_contents = section.find_elements(By.XPATH, \".//div[contains(@class, 'expand-collapse-content')]//div[@class='nt1']/div\")\n",
    "        \n",
    "                for div_content in div_contents:\n",
    "                    walmart_product_detail[div_content.find_element(By.TAG_NAME, 'h3').text.strip()] = div_content.find_element(By.TAG_NAME, \"span\").text.strip()\n",
    "\n",
    "        \n",
    "        return walmart_product_detail\n",
    "\n",
    "    def quit(self):\n",
    "        self.driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecead22d-f6fc-4b0a-8ac3-08998265c255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_contexts(text, target_word, context_size=5):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    # Find all occurrences of the target word\n",
    "    \n",
    "    target_indices = [i for i, token in enumerate(tokens) if token.lower() == target_word.lower()]\n",
    "\n",
    "    # Extract context sentences for each occurrence of the target word\n",
    "    all_contexts = []\n",
    "    for target_index in target_indices:\n",
    "        start_index = max(0, target_index - context_size)\n",
    "        end_index = min(len(tokens), target_index + context_size + 1)\n",
    "        context_words = tokens[start_index:end_index]\n",
    "        context_sentence = ' '.join(context_words)\n",
    "        all_contexts.append(context_sentence)\n",
    "\n",
    "    return all_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab155099-e050-4ebf-bbbe-62ca2cfc6ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_app_name(nlp, text):\n",
    "\n",
    "    contexts = get_all_contexts(text, \"app\")\n",
    "\n",
    "    print(contexts)\n",
    "    app_name = defaultdict(int)\n",
    "\n",
    "    for context in contexts:\n",
    "        doc = nlp(context)\n",
    "\n",
    "        #iterate through the entities\n",
    "        for ent in doc.ents:\n",
    "            name = re.sub(\"[^a-zA-Z0-9]\", \"\", ent.text.upper())\n",
    "            app_name[name]  +=1\n",
    "\n",
    "\n",
    "    return max(app_name, key=app_name.get) if app_name else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10e1bf9c-1d06-4d0b-a632-2155f4ddd259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/priyanshi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ab484d0-99d4-4464-9e8c-c8fae60a9812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_pipeline' (0.0.0) was trained with spaCy v3.6.1 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "#download punkt using nltk.download('punkt') and provide the path below\n",
    "nltk.data.path.append(\"tokenizers\")\n",
    "\n",
    "#load the model\n",
    "# nlp = spacy.load(r\"C:\\Users\\moink\\Downloads\\white_label\\model-best\")\n",
    "nlp = spacy.load(r\"case_sensitive_ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4551bf6a-2e90-4fa4-ae0d-f3ac8a65e837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1MxfBZMJTHZFUOZn96I7kO61sibVVKyo7'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_folder_id = \"1R9S8lc3N5nB0dqtIytS8g08K9Xn2NwFU\"\n",
    "\n",
    "#path to client secrets\n",
    "# You need a Oauth2.0 Client to access the GoogleDrive, follow this link https://support.google.com/cloud/answer/6158849?hl=en \n",
    "#to create client id. Download the client secret and name the file as client_secrets.json, and stored in the same directory as this notebook or set client_config_file attr\n",
    "GoogleAuth.DEFAULT_SETTINGS['client_config_file'] = \"client_secrets.json\"\n",
    "drive = GoogleDriveWrapper()\n",
    "folder_id = drive.createFolder(\"smart wifi plug\".title(), shared_folder_id)\n",
    "folder_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4a9ba2b9-e3ff-4bec-8f38-b7018fb7312e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = WalmartScraper (\"smart wifi plug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "04c4a0b7-986d-402a-b4d0-b29a9e7cfd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_urls = scraper.getProductURLList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5b218fb9-17fb-47f1-bebe-44cc29d287c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1990"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(product_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f52d2a40-f084-450c-ace6-048196c72b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_details= {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e07bb86-347c-4768-9d33-e59a77c76e46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for product_url in product_urls:\n",
    "\n",
    "    scraper.driver.get(product_url)\n",
    "    product_name, product_id = scraper.getProductNameAndIDFromURL()\n",
    "\n",
    "    print(product_name, product_id)\n",
    "    if product_id not in product_details:\n",
    "        product_details[product_id] = {'id': product_id}\n",
    "\n",
    "        product_details[product_id]['name'] = product_name\n",
    "\n",
    "        image_url = scraper.getLeftImageUrls()[0]\n",
    "        product_detail = scraper.parseWalmartProduct()\n",
    "        walmart_product_detail = scraper.parseWalmartProductDetails()\n",
    "\n",
    "\n",
    "        product_details[product_id]['brand'] = product_detail['product_brand']\n",
    "        product_details[product_id]['long_description'] = walmart_product_detail['long_description']\n",
    "        \n",
    "        description_list = [walmart_product_detail['long_description'], walmart_product_detail['short_description']]\n",
    "\n",
    "        product_details[product_id]['app_name'] = None\n",
    "        for description in description_list:\n",
    "            if description:\n",
    "                product_details[product_id]['app_name'] = get_app_name(nlp, description.replace(\"\\n\", \" \"))\n",
    "                if product_details[product_id]['app_name']:\n",
    "                    break\n",
    "\n",
    "        print(product_details[product_id]['app_name'])\n",
    "\n",
    "        image_name = f\"{product_id}_{product_name.replace(' ', '-')}.jpg\"\n",
    "        #print(image['image_url'])\n",
    "        \n",
    "        #check if file already present\n",
    "        file_id = drive.getFileID(file_name=image_name, parent_id=folder_id)\n",
    "    \n",
    "        #file not present\n",
    "        if not file_id:\n",
    "            #upload file under the parent folder with folder id\n",
    "            file_id = drive.uploadFile(file_name=image_name, file_url= image_url, parent_id=folder_id)\n",
    "\n",
    "\n",
    "        product_details[product_id]['image_url'] = image_url\n",
    "        product_details[product_id]['url'] = product_url\n",
    "        time.sleep(random.randrange(1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "65899e26-ced4-473c-bf50-61f541bdcb3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1137"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(product_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3aab20cc-082e-444b-bbae-1bcab0efd619",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(product_details.values())\n",
    "# df = df.loc[:, df.columns != 'images']\n",
    "df = df.dropna(subset=['url'])\n",
    "df.to_csv(\"walmart_products_dataset.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "953db3f7-8c18-40c6-b81b-8b14f602f1e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac0d58e-d6d2-4721-8cb8-dfbc2b66ada3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
